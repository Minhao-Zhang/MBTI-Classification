# MBTI分类器 - 可行性研究

基于文本数据对某人的MBTI类型进行分类。

There's also an [English version](./README.md) of this document.

## 数据准备

### 数据整理

为了这个项目，我创建了一个自定义数据集，并已公开共享。原始数据由 Dylan Storey 上传到 [Zenodo](https://zenodo.org/records/1482951)，使用 Google Big Query 从 Reddit 获取。数据中包含用户自我标识的 MBTI 类型。你可以通过运行以下命令来下载原始数据：

```bash
./setup.sh
```

这个命令还会为数据处理创建所需的临时目录。

数据经过了详尽的清洗，包括：
- 将所有文本转换为小写。
- 移除所有网址。
- 排除包含非英文字符的帖子。
- 去除 Reddit 特有的链接格式，如 `r/abcd` 和 `u/abcd`。
- 保留 `?` 和 `!` 之外的所有特殊字符。
- 过滤掉长度少于 20 个字符或超过 3,000 个字符的帖子。

关于清洗步骤的具体细节，请参阅 [clean_data.py](./preprocessing/clean_data.py)。

在清洗过程中，我发现了一些数据不一致的情况：
- 一些作者关联了多个 MBTI 类型。
- 有些帖子是同一作者重复发布的。

为了解决这些问题，我删除了所有重复的帖子，并排除了那些被多名版主禁用的用户发布的帖子。这样做是为了确保数据的一致性。相关操作都在 [remove_duplicate.py](./preprocessing/remove_duplicate.py) 中实现。

### 数据概览

最终的数据集包含 1,300 万行和 3 列，代表了 11,773 位唯一的作者。每一行包含一个帖子、作者名称以及 MBTI 类型。我们还提供了一个额外的数据集，其中包含唯一作者及其 MBTI 类型。

这个数据集是进行 MBTI 分类的重要资源，已经通过 [Kaggle](https://www.kaggle.com/datasets/minhaozhang1/reddit-mbti-dataset) 对外公开下载。

### 预处理

由于聊天消息的长度差异很大，特别是非常短的消息可能不足以提供准确的人格预测信息。为了解决这个问题，我将较短的消息合并，确保每条消息至少有 700 个字符（包括空格），最多 1,000 个字符。具体步骤详见 [combine_short_text.py](./preprocessing/combine_short_text.py)。

虽然字符数对创建平衡的数据集很有效，但在使用基于单词而非字符分词的 LLM 时可能会引发挑战。为了解决这个问题，我进一步处理了数据，使其具有更均匀的单词长度范围。详细步骤在 [evenout_word_length.py](./preprocessing/evenout_word_length.py) 中，你也可以通过 [Hugging Face](https://huggingface.co/datasets/minhaozhang/mbti) 获取这些数据。

### 基线分数

数据集准备好后，我们可以开始探索其中 MBTI 类型的分布情况。可以使用多数分类器来建立基线性能。更多信息请参阅 [eda.ipynb](./preprocessing/eda.ipynb)。

| 类型 | 准确度  | F1 分数 |
| ---- | ------- | ------- |
| E-I  | 0.78858 | 0.88179 |
| N-S  | 0.92603 | 0.96160 |
| F-T  | 0.53863 | 0.70014 |
| J-P  | 0.59189 | 0.74363 |

## 机器学习方法

### 前期工作

文本数据的人格特质预测已有尝试，其中最著名的是 [16personalities.com](https://www.16personalities.com/) 提供的问卷，它通过用户的回答将个体分类为 MBTI 类型。然而，从自然对话中预测个性特征则是一个更为复杂的挑战。Ryan 等人（2023年）在这一领域进行了显著的尝试，他们利用 Kaggle 的数据集来预测文本中的 MBTI 类型。

Ryan 等人采用了传统的机器学习方法，结合 TF-IDF 向量化器和 CatBoost 等分类器，并通过 SMOTE 技术平衡数据。虽然引入 SMOTE 后模型性能有所提升，但总体表现仍不理想。例如，在 I/E 二元分类中，他们取得的最佳 F1 分数为 0.8389。然而，考虑到 I/E 的数据分布（6676/1999），简单的多数投票分类器的 F1 分数就可以达到 0.86978。这表明他们的模型并未超越基本的多数投票分类器。

### 我的方法

我复现了 Ryan 等人（2023年）的研究方法，但使用了我自己整理的、更大规模的数据集。虽然我采用了相似的数据预处理策略，但尝试了多种梯度提升分类器，包括 CatBoost、XGBoost 和 LightGBM。尽管我的数据分布有所不同，但结果依然令人失望——我的最佳 F1 分数与多数分类器持平。

详细的训练和评估过程记录在 [train_model.ipynb](./ml/train_model.ipynb) 中。

这些不理想的结果可能源于 MBTI 分类的固有复杂性或传统机器学习技术的局限性。因此，我计划进一步探索大型语言模型的潜力，看看它们是否能够提升性能。

<table>
  <tr>
    <th>Type</th>
    <th>Metric</th>
    <th>Baseline</th>
    <th>XGBoost</th>
    <th>CatBoost</th>
    <th>LightGBM</th>
  </tr>
  <tr>
    <td rowspan="2">E-I</td>
    <td>Accuracy</td>
    <td>0.7886</td>
    <td>0.7891</td>
    <td>0.7889</td>
    <td>0.7890</td>
  </tr>
  <tr>
    <td>F1 Score</td>
    <td>0.8818</td>
    <td>0.8819</td>
    <td>0.8820</td>
    <td>0.8820</td>
  </tr>
  <tr>
    <td rowspan="2">N-S</td>
    <td>Accuracy</td>
    <td>0.9260</td>
    <td>0.9263</td>
    <td>0.9261</td>
    <td>0.9262</td>
  </tr>
  <tr>
    <td>F1 Score</td>
    <td>0.9616</td>
    <td>0.9617</td>
    <td>0.9616</td>
    <td>0.9617</td>
  </tr>
  <tr>
    <td rowspan="2">F-T</td>
    <td>Accuracy</td>
    <td>0.5386</td>
    <td>0.6284</td>
    <td>0.6248</td>
    <td>0.6234</td>
  </tr>
  <tr>
    <td>F1 Score</td>
    <td>0.7001</td>
    <td>0.6794</td>
    <td>0.6794</td>
    <td>0.6781</td>
  </tr>
  <tr>
    <td rowspan="2">J-P</td>
    <td>Accuracy</td>
    <td>0.5919</td>
    <td>0.6162</td>
    <td>0.6116</td>
    <td>0.6116</td>
  </tr>
  <tr>
    <td>F1 Score</td>
    <td>0.7436</td>
    <td>0.3248</td>
    <td>0.2596</td>
    <td>0.2619</td>
  </tr>
</table>

## 大型语言模型方法

### 项目启动

在这个项目中，我决定采用微软最新发布的 [Phi-3](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) 模型。尽管 Phi-3 最近已在 Azure AI Studio 中提供微调功能，但我选择使用 Hugging Face 的 [Transformers](https://huggingface.co/transformers/) 库，因为它在微调方面提供了更多的灵活性。

首先，我参考了 Hugging Face 的[序列分类教程](https://huggingface.co/docs/transformers/en/tasks/sequence_classification)，该教程演示了如何使用 Google 的 BERT 模型进行微调。虽然教程为微调提供了一个清晰的框架，但我用更先进的 Phi-3 模型替换了 BERT。然而，这也带来了更高的计算需求。即使是最小的 [Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) 模型，在我的电脑上进行微调也是不现实的。为了解决这个问题，我借助了配备 A100 GPU 的云服务来完成微调过程。

### 减少GPU内存消耗

尽管 A100 GPU 性能强大，但我仍需采用一些策略来减少 vRAM 的使用量。通过 Hugging Face 的 [Model Memory Estimator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)，我发现使用模型、`Adam` 优化器和 `float32` 数据类型组合时，所需的峰值 vRAM 达到 57GB，加上数据占用，这将超出 A100 GPU 的 80GB vRAM 容量。为此，我采取了多种策略来优化内存使用。

这些策略均借鉴自 [Transformers](https://huggingface.co/docs/transformers/perf_train_gpu_one) 教程。

| 方法/工具               | 我采用的策略                             |
| ----------------------- | ---------------------------------------- |
| 批次大小调整            | 是，以减少 vRAM 使用量                   |
| 梯度累积                | 是，有效增加批次大小                     |
| 梯度检查点              | 否，因训练速度会降低 20%                 |
| 混合精度训练            | 是，使用 `tf32` 提高训练速度             |
| torch_empty_cache_steps | 否，因训练速度会降低 10%                 |
| 优化器选择              | 是，采用 `adamw_bnb_8bit` 以减少内存使用 |
| 数据预加载              | 是，按默认配置进行                       |
| DeepSpeed Zero          | 否，无法配置环境                         |
| torch.compile           | 否，无法配置环境                         |
| 参数效率微调（PEFT）    | 否，无法配置环境                         |

有趣的是，当我尝试使用 `bf16` 数据类型进一步减少内存占用时，模型的损失输出变成了 `nan`。在 Hugging Face 论坛上，我发现了一个类似问题的讨论（[链接](https://discuss.huggingface.co/t/training-loss-0-0-validation-loss-nan/27950)），虽然讨论的是不同的模型。根据 [config.json](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/config.json#L31) 文件，模型是使用 `bf16` 数据类型进行训练的，但这并未解决问题。我还在寻找解决方案，并欢迎任何见解。

### 微调大型语言模型

基于情绪分析教程，我修改了代码以适应 MBTI 分类任务。由于 MBTI 可以分解为四个二元分类任务，我将其视为四个独立的二元分类任务，而非单一的 16 类分类任务。我从 J-P 类型的分类开始，因为它的分布相对平衡，尽管稍有偏斜。使用训练集的 1% 作为起点，我开始对模型进行微调。最初的结果表明，模型在学习上有一定进展。

![屏幕截图](./figs/run_screenshot_0.png)

模型的准确度超过了多数分类器的基线。然而，这可能仅是偶然结果。优化代码后（我意识到自己没有正确使用 `tf32`），并将训练集大小增加到 10% 时，结果不如人意。

![屏幕截图](./figs/run_screenshot_1.png)

虽然微调后的模型在识别 J-P 类型上表现出了一定能力，但仍未超越多数分类器，表明它可能仅学会了预测多数类别。意识到这个问题后，我暂停了训练，准备进一步诊断问题。

### 添加系统提示

在这一阶段，我意识到MBTI类型的分类对大型语言模型来说可能比情感分析等简单任务更具挑战性。这种分类任务的复杂性需要模型具备更细致的理解能力。

为了帮助模型更好地完成任务，我在每组输入文本前添加了一个半自动化的系统提示：

```
你是一位MBTI专家，请尝试从以下文字中识别个性类型。
此任务中仅预测判断-感知（J-P）。预测 J 为 0，P 为 1。
专注于识别人格特征，而不是数据比例。以下是你看到的文字：
```

我希望这个提示能引导模型关注关键点。然而，遗憾的是，这一调整并未显著提升模型的性能。

### 尝试其他MBTI维度

由于J-P分类的挑战性较大，我决定尝试其他MBTI维度。接下来选择了情感-逻辑（F-T）维度，这是分布最均衡的一个。最初，我没有在输入中包含系统提示。

我从训练集的5%开始对模型进行微调：

![屏幕截图](./figs/run_screenshot_2.png)

尽管模型的准确率有所提升，但F1分数依然较低。在大约8000步时，模型的准确率和F1分数都显著下降，这与新一轮训练的开始相吻合，表明模型正在重复训练相同的数据。到9000步时，模型的训练损失降得很低，而验证损失依然很高，表现出明显的过拟合现象。

这表明限制训练到单个周期可能更为必要，因为当数据被再次引入时，模型容易出现过拟合。

我在其他MBTI维度上进行的训练尝试也得到了类似的结果。

### 数据再处理

最初，为了便于训练word2vec模型，我将文本转换为小写，并去除了所有标点符号（除了问号和感叹号）。此外，数据集中每个帖子的平均字数约为200个单词，这可能不足以让大型语言模型识别出言语模式。

因此，我增加了每个帖子的字数至400个词，尽管这减少了样本的总数量。数据通过[re_process_data.ipynb](./preprocessing/re_process_data.ipynb)重新处理，并再次对模型进行了微调。

然而，即使进行了这些改进，模型的表现仍然不及简单的多数分类器。

## 处理不平衡数据

### 概览

不平衡数据在分类任务中是一个常见挑战，即使是强大的模型如大型语言模型（LLM）也不例外。当面对不平衡数据时，模型往往倾向于预测占多数的类别，从而在少数类别上的性能表现不佳。

为了解决这一问题，可以采用多种方法：

- **过采样少数类别 / 下采样多数类别**
- **SMOTE（合成少数过采样技术）**
- **类别权重平衡**

### 对多数类别进行下采样

在前文中，我观察到模型在重新引入训练数据时出现了过拟合现象。考虑到这一点，我选择了对多数类别进行下采样，而非过度采样少数类别，因为后者可能会加剧过拟合。由于数据量足够大，我可以在不影响总体数据集规模的情况下减少多数类别的数量。

一种简单的下采样方法是随机选择多数类别的子集，使其大小与少数类别相匹配。

以J-P维度为例，虽然该维度存在一定的不平衡，但并不极端。我在从训练子集中分离出额外验证集后，对多数类别进行了下采样。然而，这一策略并未显著提升模型的性能。尽管训练损失发生了变化，表明数据分布有所不同，但模型的准确率和F1分数与多数分类器相比保持不变。

这表明我的下采样实现可能不足以解决问题，或者MBTI分类任务本身对这种方法提出了独特的挑战。

### 合成少数过采样技术（SMOTE）

SMOTE通过生成少数类别的合成样本来增加少数类别的数据量，可以通过`imbalanced-learn`库实现。在[Hugging Face论坛](https://discuss.huggingface.co/t/how-to-apply-smote-to-a-dataset/27876)中可以找到关于在文本数据上使用SMOTE的讨论。

然而，基于我的研究，我得出结论，SMOTE可能不适合用于生成合成文本数据。尽管Ryan等人（2023年）在他们的研究中使用了SMOTE并观察到了性能提升，但他们的模型仍无法超越多数分类器。

这促使我考虑其他处理不平衡数据的方法。

### 类别权重平衡

我还研究了另一种技术：类别权重平衡。根据[Hugging Face论坛](https://discuss.huggingface.co/t/how-can-i-use-class-weights-when-training/1067)中的讨论，此方法通过将与少数类别相关的损失权重设置为多数类别对少数类别比例的逆向权重，从而调整模型训练。

对于J-P维度，该维度的类别比例大约为60%对40%，我根据这个比例设置了类别权重。

遗憾的是，这种方法并未显著提升模型性能。虽然在训练初期，损失显示出模型试图从文本中学习模式，但很快开始一致地预测多数类别。

然而，有一个积极的发展：

![截图](./figs/run_screenshot_3.png)

随着训练规模的增加，模型开始显示出一定的学习迹象。模型的准确率超过了多数分类器，这是一个积极的信号。我计划继续训练这个模型，期待在进一步训练中能有更显著的提升。

## 思考与未来工作

### 思考

基于文本数据的MBTI分类具有独特的挑战，与情感分析等更为直观的任务相比，这一任务要求更深入的理解。情感分析通常可以通过特定的关键词或短语直接获得线索，而MBTI分类则需要对上下文、语言习惯和说话者意图进行细致的分析。这种微妙性使得即使是先进的模型也难以准确预测个性类型。MBTI分类要求对语言和行为有更深刻的理解，而现有的LLM可能尚不足以完全应对这一挑战。

### 未来工作

在该项目中，有几个可以改进的领域：

1. **使用更丰富且更长的文本数据集**：
   - 这个项目中，每篇帖子被限制在200-400个字以内，以保持足够的数据量。然而，较长的文本可能提供更多的上下文信息，从而提升模型的表现。尽管将短文组合成长文会减少样本数量，但如果同一作者撰写了多篇长文，可能会引入偏见。因此，使用更长篇幅的文章和更丰富多样的数据集可能更有助于提高模型的表现。

2. **使用更强大的LLM模型**：
   - 虽然Phi-3是一个相对较新且有前途的模型，但其参数量（38亿）和上下文长度（4k tokens）仍然有限。更大的模型如Llama3.1，拥有高达4050亿参数和128k tokens的上下文长度，可能更适合处理MBTI分类任务。这些大型模型能够从文本中提取出更为细致的信息，并可能提供更好的性能。遗憾的是，由于计算资源的限制，在这个项目中对如此庞大的模型进行微调是不现实的。

3. **采用更优化的微调策略**：
   - 目前的方法是通过Hugging Face Transformers库中的`AutoModelForSequenceClassification`进行微调，这是一个良好的起点。然而，使用PyTorch的自定义训练循环可能提供更多灵活性，使得微调策略能够更好地应对MBTI分类的具体挑战。

4. **应用更高级的数据预处理策略**：
   - 该项目中采用的基本数据预处理策略相对简单，但可能并非最佳实践。虽然使用了常见的NLP技术，但可以开发更为复杂的预处理方法，结合心理学和语言学的见解。通过更细致地处理语言结构和上下文元素，模型可能会更好地识别和分类个性类型的微妙差异。

## 参考文献

Ryan, Gregorius, Pricillia Katarina, and Derwin Suhartono. 2023. "MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences" Information 14, no. 4: 217. https://doi.org/10.3390/info14040217